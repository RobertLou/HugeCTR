{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86a1bd",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-hps-table-fusion-demo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HPS Table Fusion Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f41a5",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to fuse embedding tables of the same embedding vector size with the HPS plugin for TensorFlow. It is recommended to run [hierarchical_parameter_server_demo.ipynb](hierarchical_parameter_server_demo.ipynb) before diving into this notebook.\n",
    "\n",
    "For more details about HPS APIs, please refer to [HPS APIs](https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/api/index.html). For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa300270",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get HPS from NGC\n",
    "\n",
    "The HPS Python module is preinstalled in the 23.12 and later [Merlin TensorFlow Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow): `nvcr.io/nvidia/merlin/merlin-tensorflow:23.12`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import hierarchical_parameter_server as hps\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e5190",
   "metadata": {},
   "source": [
    "## Create TF SavedModel\n",
    "\n",
    "First of all we specify the required configurations, e.g., the arguments needed for generating the embedding tables, the template HPS JSON configuration file. We will use a naive deep neural network (DNN) model which has 8 embedding tables of the same emebedding vector size and one fully connected layer in this notebook. \n",
    "\n",
    "We define the model with `hps.LookupLayer` and some native TF layers, and then save it in the SavedModel format. Please note that the table fusion is turned off here by setting `fuse_embedding_table` as `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d06656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_model_for_table_fusion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_model_for_table_fusion.py\n",
    "\n",
    "import hierarchical_parameter_server as hps\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import struct\n",
    "import json\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "NUM_GPUS = 1\n",
    "VOCAB_SIZE = 10000\n",
    "EMB_VEC_SIZE = 128\n",
    "NUM_QUERY_KEY = 26\n",
    "EMB_VEC_DTYPE = np.float32\n",
    "TF_KEY_TYPE = tf.int32\n",
    "MAX_BATCH_SIZE = 256\n",
    "NUM_ITERS = 100\n",
    "NUM_TABLES = 8\n",
    "USE_CONTEXT_STREAM = True\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(NUM_GPUS)))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "hps_config = {\n",
    "    \"supportlonglong\": False,\n",
    "    \"fuse_embedding_table\": True,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model\": str(NUM_TABLES) + \"_table\",\n",
    "            \"sparse_files\": [],\n",
    "            \"num_of_worker_buffer_in_pool\": NUM_TABLES,\n",
    "            \"embedding_table_names\": [],\n",
    "            \"embedding_vecsize_per_table\": [],\n",
    "            \"maxnum_catfeature_query_per_table_per_sample\": [],\n",
    "            \"default_value_for_each_table\": [0.0],\n",
    "            \"deployed_device_list\": [0],\n",
    "            \"max_batch_size\": MAX_BATCH_SIZE,\n",
    "            \"cache_refresh_percentage_per_iteration\": 1.0,\n",
    "            \"hit_rate_threshold\": 1.0,\n",
    "            \"gpucacheper\": 1.0,\n",
    "            \"gpucache\": True,\n",
    "            \"embedding_cache_type\": \"dynamic\",\n",
    "            \"use_context_stream\": True,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "def generate_embedding_tables(hugectr_sparse_model, vocab_range, embedding_vec_size):\n",
    "    os.system(\"mkdir -p {}\".format(hugectr_sparse_model))\n",
    "    with open(\"{}/key\".format(hugectr_sparse_model), \"wb\") as key_file, open(\n",
    "        \"{}/emb_vector\".format(hugectr_sparse_model), \"wb\"\n",
    "    ) as vec_file:\n",
    "        for key in range(vocab_range[0], vocab_range[1]):\n",
    "            vec = 0.00025 * np.ones((embedding_vec_size,)).astype(np.float32)\n",
    "            key_struct = struct.pack(\"q\", key)\n",
    "            vec_struct = struct.pack(str(embedding_vec_size) + \"f\", *vec)\n",
    "            key_file.write(key_struct)\n",
    "            vec_file.write(vec_struct)\n",
    "\n",
    "\n",
    "def set_up_model_files():\n",
    "    for i in range(NUM_TABLES):\n",
    "        table_name = \"table\" + str(i)\n",
    "        model_file_name = \"embeddings/\" + table_name\n",
    "        generate_embedding_tables(\n",
    "            model_file_name, [i * VOCAB_SIZE, (i + 1) * VOCAB_SIZE], EMB_VEC_SIZE\n",
    "        )\n",
    "        hps_config[\"models\"][0][\"sparse_files\"].append(model_file_name)\n",
    "        hps_config[\"models\"][0][\"embedding_table_names\"].append(table_name)\n",
    "        hps_config[\"models\"][0][\"embedding_vecsize_per_table\"].append(EMB_VEC_SIZE)\n",
    "        hps_config[\"models\"][0][\"maxnum_catfeature_query_per_table_per_sample\"].append(\n",
    "            NUM_QUERY_KEY\n",
    "        )\n",
    "    return hps_config\n",
    "\n",
    "class InferenceModel(tf.keras.models.Model):\n",
    "    def __init__(self, num_tables, **kwargs):\n",
    "        super(InferenceModel, self).__init__(**kwargs)\n",
    "        self.lookup_layers = []\n",
    "        for i in range(num_tables):\n",
    "            self.lookup_layers.append(\n",
    "                hps.LookupLayer(\n",
    "                    model_name=str(NUM_TABLES) + \"_table\",\n",
    "                    table_id=i,\n",
    "                    emb_vec_size=EMB_VEC_SIZE,\n",
    "                    emb_vec_dtype=EMB_VEC_DTYPE,\n",
    "                    ps_config_file=str(NUM_TABLES) + \"_table.json\",\n",
    "                    global_batch_size=MAX_BATCH_SIZE,\n",
    "                    name=\"embedding_lookup\" + str(i),\n",
    "                )\n",
    "            )\n",
    "        self.fc = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=\"ones\",\n",
    "            bias_initializer=\"zeros\",\n",
    "            name=\"fc\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        assert len(inputs) == len(self.lookup_layers)\n",
    "        embeddings = []\n",
    "        for i in range(len(inputs)):\n",
    "            embeddings.append(\n",
    "                tf.reshape(\n",
    "                    self.lookup_layers[i](inputs[i]), shape=[-1, NUM_QUERY_KEY * EMB_VEC_SIZE]\n",
    "                )\n",
    "            )\n",
    "        concat_embeddings = tf.concat(embeddings, axis=1)\n",
    "        logit = self.fc(concat_embeddings)\n",
    "        return logit\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = []\n",
    "        for _ in range(len(self.lookup_layers)):\n",
    "            inputs.append(tf.keras.Input(shape=(NUM_QUERY_KEY,), dtype=TF_KEY_TYPE))\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()\n",
    "\n",
    "\n",
    "def create_savedmodel(hps_config):\n",
    "    # Overwrite JSON configuration file\n",
    "    hps_config[\"fuse_embedding_table\"] = False\n",
    "    hps_config_json_object = json.dumps(hps_config, indent=4)\n",
    "    with open(str(NUM_TABLES) + \"_table.json\", \"w\") as outfile:\n",
    "        outfile.write(hps_config_json_object)\n",
    "\n",
    "    model = InferenceModel(NUM_TABLES)\n",
    "    model.summary()\n",
    "    inputs = []\n",
    "    for i in range(NUM_TABLES):\n",
    "        inputs.append(\n",
    "            np.random.randint(\n",
    "                i * VOCAB_SIZE, (i + 1) * VOCAB_SIZE, (MAX_BATCH_SIZE, NUM_QUERY_KEY)\n",
    "            ).astype(np.int32)\n",
    "        )\n",
    "    model(inputs)\n",
    "    model.save(str(NUM_TABLES) + \"_table.savedmodel\")\n",
    "\n",
    "    # Overwrite JSON configuration file\n",
    "    hps_config[\"fuse_embedding_table\"] = True\n",
    "    hps_config_json_object = json.dumps(hps_config, indent=4)\n",
    "    with open(str(NUM_TABLES) + \"_table.json\", \"w\") as outfile:\n",
    "        outfile.write(hps_config_json_object)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hps_config = set_up_model_files()\n",
    "    create_savedmodel(hps_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de99fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 07:24:28.206281: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 07:24:36.420084: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 07:24:36.926162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30996 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] hierarchical_parameter_server is imported\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_lookup0 (LookupLayer  (None, 26, 128)     0           ['input_1[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_lookup1 (LookupLayer  (None, 26, 128)     0           ['input_2[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_lookup2 (LookupLayer  (None, 26, 128)     0           ['input_3[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_lookup3 (LookupLayer  (None, 26, 128)     0           ['input_4[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_lookup4 (LookupLayer  (None, 26, 128)     0           ['input_5[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_lookup5 (LookupLayer  (None, 26, 128)     0           ['input_6[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_lookup6 (LookupLayer  (None, 26, 128)     0           ['input_7[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_lookup7 (LookupLayer  (None, 26, 128)     0           ['input_8[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 3328)         0           ['embedding_lookup0[0][0]']      \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)      (None, 3328)         0           ['embedding_lookup1[0][0]']      \n",
      "                                                                                                  \n",
      " tf.reshape_2 (TFOpLambda)      (None, 3328)         0           ['embedding_lookup2[0][0]']      \n",
      "                                                                                                  \n",
      " tf.reshape_3 (TFOpLambda)      (None, 3328)         0           ['embedding_lookup3[0][0]']      \n",
      "                                                                                                  \n",
      " tf.reshape_4 (TFOpLambda)      (None, 3328)         0           ['embedding_lookup4[0][0]']      \n",
      "                                                                                                  \n",
      " tf.reshape_5 (TFOpLambda)      (None, 3328)         0           ['embedding_lookup5[0][0]']      \n",
      "                                                                                                  \n",
      " tf.reshape_6 (TFOpLambda)      (None, 3328)         0           ['embedding_lookup6[0][0]']      \n",
      "                                                                                                  \n",
      " tf.reshape_7 (TFOpLambda)      (None, 3328)         0           ['embedding_lookup7[0][0]']      \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 26624)        0           ['tf.reshape[0][0]',             \n",
      "                                                                  'tf.reshape_1[0][0]',           \n",
      "                                                                  'tf.reshape_2[0][0]',           \n",
      "                                                                  'tf.reshape_3[0][0]',           \n",
      "                                                                  'tf.reshape_4[0][0]',           \n",
      "                                                                  'tf.reshape_5[0][0]',           \n",
      "                                                                  'tf.reshape_6[0][0]',           \n",
      "                                                                  'tf.reshape_7[0][0]']           \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 1)            26625       ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,625\n",
      "Trainable params: 26,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][07:24:38.079][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][07:24:38.079][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][07:24:38.079][INFO][RK0][main]: num_of_refresher_buffer_in_pool is not specified using default: 1\n",
      "[HCTR][07:24:38.079][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][07:24:38.079][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][07:24:38.079][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "[HCTR][07:24:38.079][INFO][RK0][main]: use_static_table is not specified using default: 0\n",
      "[HCTR][07:24:38.079][INFO][RK0][main]: HPS plugin uses context stream for model 8_table: True\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][07:24:38.080][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][07:24:38.080][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][07:24:38.080][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][07:24:38.080][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][07:24:38.080][DEBUG][RK0][main]: Created raw model loader in local memory!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][07:24:38.547][INFO][RK0][main]: Table: hps_et.8_table.table0; cached 10000 / 10000 embeddings in volatile database (HashMapBackend); load: 10000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:24:39.379][INFO][RK0][main]: Table: hps_et.8_table.table1; cached 10000 / 10000 embeddings in volatile database (HashMapBackend); load: 10000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:24:39.830][INFO][RK0][main]: Table: hps_et.8_table.table2; cached 10000 / 10000 embeddings in volatile database (HashMapBackend); load: 10000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:24:40.448][INFO][RK0][main]: Table: hps_et.8_table.table3; cached 10000 / 10000 embeddings in volatile database (HashMapBackend); load: 10000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:24:40.899][INFO][RK0][main]: Table: hps_et.8_table.table4; cached 10000 / 10000 embeddings in volatile database (HashMapBackend); load: 10000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:24:41.934][INFO][RK0][main]: Table: hps_et.8_table.table5; cached 10000 / 10000 embeddings in volatile database (HashMapBackend); load: 10000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:24:43.097][INFO][RK0][main]: Table: hps_et.8_table.table6; cached 10000 / 10000 embeddings in volatile database (HashMapBackend); load: 10000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:24:45.296][INFO][RK0][main]: Table: hps_et.8_table.table7; cached 10000 / 10000 embeddings in volatile database (HashMapBackend); load: 10000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:24:45.296][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][07:24:45.297][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: Model name: 8_table\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: Max batch size: 256\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: Fuse embedding tables: False\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: Number of embedding tables: 8\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: Embedding cache type: dynamic\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: Use I64 input key: False\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: The size of worker memory pool: 8\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][07:24:45.306][INFO][RK0][main]: The refresh percentage : 1.000000\n",
      "[HCTR][07:24:45.469][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][07:24:45.470][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.table0\n",
      "[HCTR][07:24:45.470][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.table1\n",
      "[HCTR][07:24:45.470][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.table2\n",
      "[HCTR][07:24:45.470][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.table3\n",
      "[HCTR][07:24:45.470][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.table4\n",
      "[HCTR][07:24:45.470][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.table5\n",
      "[HCTR][07:24:45.470][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.table6\n",
      "[HCTR][07:24:45.470][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.table7\n",
      "[HCTR][07:24:45.475][INFO][RK0][main]: LookupSession i64_input_key: False\n",
      "[HCTR][07:24:45.475][INFO][RK0][main]: Creating lookup session for 8_table on device: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"python3 create_model_for_table_fusion.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a187288",
   "metadata": {},
   "source": [
    "## Make inference with HPS table fusion\n",
    "\n",
    "We load the TF SavedModel and make inference for several batches. The table fusion is enabled since `fuse_embedding_table` is `True` within the HPS JSON configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dea6b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] hierarchical_parameter_server is imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 07:25:39.918038: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 07:25:42.325440: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 07:25:42.818316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30996 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: Table fusion is enabled for HPS. Please ensure that there is no key value overlap in different tables and the embedding lookup layer has no dependency in the model graph. For more information, see https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/hps_database_backend.html#configuration\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][07:25:43.756][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: num_of_refresher_buffer_in_pool is not specified using default: 1\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: use_static_table is not specified using default: 0\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: HPS plugin uses context stream for model 8_table: True\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][07:25:43.756][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][07:25:43.756][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][07:25:43.756][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][07:25:44.292][INFO][RK0][main]: Table: hps_et.8_table.fused_embedding0; cached 80000 / 80000 embeddings in volatile database (HashMapBackend); load: 80000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:25:44.292][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][07:25:44.292][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: Model name: 8_table\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: Max batch size: 256\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: Fuse embedding tables: True\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: Embedding cache type: dynamic\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: Use I64 input key: False\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: The size of worker memory pool: 8\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][07:25:44.299][INFO][RK0][main]: The refresh percentage : 1.000000\n",
      "[HCTR][07:25:44.406][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][07:25:44.406][INFO][RK0][main]: EC initialization on device 0 for hps_et.8_table.fused_embedding0\n",
      "[HCTR][07:25:44.407][INFO][RK0][main]: LookupSession i64_input_key: False\n",
      "[HCTR][07:25:44.407][INFO][RK0][main]: Creating lookup session for 8_table on device: 0\n",
      "-------------------- Step 0 --------------------\n",
      "-------------------- Step 1 --------------------\n",
      "-------------------- Step 2 --------------------\n",
      "-------------------- Step 3 --------------------\n",
      "-------------------- Step 4 --------------------\n",
      "-------------------- Step 5 --------------------\n",
      "-------------------- Step 6 --------------------\n",
      "-------------------- Step 7 --------------------\n",
      "-------------------- Step 8 --------------------\n",
      "-------------------- Step 9 --------------------\n",
      "-------------------- Step 10 --------------------\n",
      "-------------------- Step 11 --------------------\n",
      "-------------------- Step 12 --------------------\n",
      "-------------------- Step 13 --------------------\n",
      "-------------------- Step 14 --------------------\n",
      "-------------------- Step 15 --------------------\n",
      "-------------------- Step 16 --------------------\n",
      "-------------------- Step 17 --------------------\n",
      "-------------------- Step 18 --------------------\n",
      "-------------------- Step 19 --------------------\n",
      "-------------------- Step 20 --------------------\n",
      "-------------------- Step 21 --------------------\n",
      "-------------------- Step 22 --------------------\n",
      "-------------------- Step 23 --------------------\n",
      "-------------------- Step 24 --------------------\n",
      "-------------------- Step 25 --------------------\n",
      "-------------------- Step 26 --------------------\n",
      "-------------------- Step 27 --------------------\n",
      "-------------------- Step 28 --------------------\n",
      "-------------------- Step 29 --------------------\n",
      "-------------------- Step 30 --------------------\n",
      "-------------------- Step 31 --------------------\n",
      "-------------------- Step 32 --------------------\n",
      "-------------------- Step 33 --------------------\n",
      "-------------------- Step 34 --------------------\n",
      "-------------------- Step 35 --------------------\n",
      "-------------------- Step 36 --------------------\n",
      "-------------------- Step 37 --------------------\n",
      "-------------------- Step 38 --------------------\n",
      "-------------------- Step 39 --------------------\n",
      "-------------------- Step 40 --------------------\n",
      "-------------------- Step 41 --------------------\n",
      "-------------------- Step 42 --------------------\n",
      "-------------------- Step 43 --------------------\n",
      "-------------------- Step 44 --------------------\n",
      "-------------------- Step 45 --------------------\n",
      "-------------------- Step 46 --------------------\n",
      "-------------------- Step 47 --------------------\n",
      "-------------------- Step 48 --------------------\n",
      "-------------------- Step 49 --------------------\n",
      "-------------------- Step 50 --------------------\n",
      "-------------------- Step 51 --------------------\n",
      "-------------------- Step 52 --------------------\n",
      "-------------------- Step 53 --------------------\n",
      "-------------------- Step 54 --------------------\n",
      "-------------------- Step 55 --------------------\n",
      "-------------------- Step 56 --------------------\n",
      "-------------------- Step 57 --------------------\n",
      "-------------------- Step 58 --------------------\n",
      "-------------------- Step 59 --------------------\n",
      "-------------------- Step 60 --------------------\n",
      "-------------------- Step 61 --------------------\n",
      "-------------------- Step 62 --------------------\n",
      "-------------------- Step 63 --------------------\n",
      "-------------------- Step 64 --------------------\n",
      "-------------------- Step 65 --------------------\n",
      "-------------------- Step 66 --------------------\n",
      "-------------------- Step 67 --------------------\n",
      "-------------------- Step 68 --------------------\n",
      "-------------------- Step 69 --------------------\n",
      "-------------------- Step 70 --------------------\n",
      "-------------------- Step 71 --------------------\n",
      "-------------------- Step 72 --------------------\n",
      "-------------------- Step 73 --------------------\n",
      "-------------------- Step 74 --------------------\n",
      "-------------------- Step 75 --------------------\n",
      "-------------------- Step 76 --------------------\n",
      "-------------------- Step 77 --------------------\n",
      "-------------------- Step 78 --------------------\n",
      "-------------------- Step 79 --------------------\n",
      "-------------------- Step 80 --------------------\n",
      "-------------------- Step 81 --------------------\n",
      "-------------------- Step 82 --------------------\n",
      "-------------------- Step 83 --------------------\n",
      "-------------------- Step 84 --------------------\n",
      "-------------------- Step 85 --------------------\n",
      "-------------------- Step 86 --------------------\n",
      "-------------------- Step 87 --------------------\n",
      "-------------------- Step 88 --------------------\n",
      "-------------------- Step 89 --------------------\n",
      "-------------------- Step 90 --------------------\n",
      "-------------------- Step 91 --------------------\n",
      "-------------------- Step 92 --------------------\n",
      "-------------------- Step 93 --------------------\n",
      "-------------------- Step 94 --------------------\n",
      "-------------------- Step 95 --------------------\n",
      "-------------------- Step 96 --------------------\n",
      "-------------------- Step 97 --------------------\n",
      "-------------------- Step 98 --------------------\n",
      "-------------------- Step 99 --------------------\n",
      "[INFO] Elapsed time for 100 iterations: 0.9442901611328125 seconds\n"
     ]
    }
   ],
   "source": [
    "import hierarchical_parameter_server as hps\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "NUM_GPUS = 1\n",
    "VOCAB_SIZE = 10000\n",
    "NUM_QUERY_KEY = 26\n",
    "MAX_BATCH_SIZE = 256\n",
    "NUM_ITERS = 100\n",
    "NUM_TABLES = 8\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(NUM_GPUS)))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model(str(NUM_TABLES) + \"_table.savedmodel\")\n",
    "inputs_seq = []\n",
    "for _ in range(NUM_ITERS + 1):\n",
    "    inputs = []\n",
    "    for i in range(NUM_TABLES):\n",
    "        inputs.append(\n",
    "            np.random.randint(\n",
    "                i * VOCAB_SIZE, (i + 1) * VOCAB_SIZE, (MAX_BATCH_SIZE, NUM_QUERY_KEY)\n",
    "            ).astype(np.int32)\n",
    "        )\n",
    "    inputs_seq.append(inputs)\n",
    "preds = model(inputs_seq[0])\n",
    "start = time.time()\n",
    "for i in range(NUM_ITERS):\n",
    "    print(\"-\" * 20, \"Step {}\".format(i), \"-\" * 20)\n",
    "    preds = model(inputs_seq[i + 1])\n",
    "end = time.time()\n",
    "print(\n",
    "    \"[INFO] Elapsed time for \"\n",
    "    + str(NUM_ITERS)\n",
    "    + \" iterations: \"\n",
    "    + str(end - start)\n",
    "    + \" seconds\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
