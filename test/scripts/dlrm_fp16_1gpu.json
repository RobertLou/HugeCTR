{
  "solver": {
    "lr_policy": "fixed",
    "display": 200,
    "max_iter": 18000,
    "gpu": [
      0
    ],
    "batchsize": 2048,
    "snapshot": 10000000,
    "snapshot_prefix": "./",
    "eval_interval": 1000,
    "max_eval_batches": 2048,
    "mixed_precision": 1024,
    "input_key_type": "I64"
  },
  "optimizer": {
    "type": "Adam",
    "update_type": "Global",
    "adam_hparam": {
      "learning_rate": 0.001,
      "beta1": 0.9,
      "beta2": 0.999,
      "epsilon": 0.0001
    }
  },
  "layers": [
    {
      "name": "data",
      "type": "Data",
      "format": "Parquet",
      "source": "./_file_list.txt",
      "eval_source": "./_file_list.txt",
      "check": "None",
      "slot_size_array": [381808, 22456, 14763, 7118, 19308, 4, 6443, 1259, 54, 341642, 112151, 94957, 11, 2188, 8399, 61, 4, 949, 15, 382633, 246818, 370704, 92823, 9773, 78, 34],
      "label": {
        "top": "label",
        "label_dim": 1
      },
      "dense": {
        "top": "dense",
        "dense_dim": 13
      },
      "sparse": [
        {
          "top": "data1",
          "slot_num": 26,
          "is_fixed_length": false,
          "nnz_per_slot": 1
        }
      ]
    },
    {
      "name": "sparse_embedding1",
      "type": "LocalizedSlotSparseEmbeddingHash",
      "bottom": "data1",
      "top": "sparse_embedding1",
      "sparse_embedding_hparam": {
        "embedding_vec_size": 128,
        "combiner": "sum",
        "workspace_size_per_gpu_in_mb": 3960
      }
    },
    {
      "bottom": "dense",
      "mlp_param": {
        "activation": "Relu",
        "num_output": 1,
        "num_outputs": [
          512,
          256,
          128
        ],
        "use_bias": true
      },
      "top": "mlp1",
      "type": "MLP"
    },
    {
      "bottom": [
        "mlp1",
        "sparse_embedding1"
      ],
      "top": [
        "interaction1",
        "interaction_grad"
      ],
      "type": "Interaction"
    },
    {
      "bottom": [
        "interaction1",
        "interaction_grad"
      ],
      "mlp_param": {
        "activations": [
          "Relu",
          "Relu",
          "Relu",
          "Relu",
          "None"
        ],
        "num_output": 1,
        "num_outputs": [
          1024,
          1024,
          512,
          256,
          1
        ],
        "use_bias": true
      },
      "top": "mlp2",
      "type": "MLP"
    },
    {
      "bottom": [
        "mlp2",
        "label"
      ],
      "top": "loss",
      "type": "BinaryCrossEntropyLoss"
    }
  ]
}