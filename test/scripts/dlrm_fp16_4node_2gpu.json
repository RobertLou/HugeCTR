{
  "solver": {
    "lr_policy": "fixed",
    "display": 200,
    "max_iter": 3000,
    "gpu": [
      [
        0,
        1
      ],
      [
        2,
        3
      ],
      [
        4,
        5
      ],
      [
        6,
        7
      ]
    ],
    "batchsize": 16384,
    "snapshot": 10000000,
    "snapshot_prefix": "./",
    "eval_interval": 1000,
    "max_eval_batches": 2048,
    "mixed_precision": 1024,
    "export_predictions_prefix": "/workdir/export_predictions_dlrm_fp16_4node_2gpu/"
  },
  "optimizer": {
    "type": "Adam",
    "update_type": "Global",
    "adam_hparam": {
      "learning_rate": 0.001,
      "beta1": 0.9,
      "beta2": 0.999,
      "epsilon": 0.0001
    }
  },
  "layers": [
    {
      "name": "data",
      "type": "Data",
      "source": "./file_list.txt",
      "eval_source": "./file_list_test.txt",
      "check": "Sum",
      "label": {
        "top": "label",
        "label_dim": 1
      },
      "dense": {
        "top": "dense",
        "dense_dim": 13
      },
      "sparse": [
        {
          "top": "data1",
          "slot_num": 26,
          "is_fixed_length": false,
          "nnz_per_slot": 1
        }
      ]
    },
    {
      "name": "sparse_embedding1",
      "type": "LocalizedSlotSparseEmbeddingHash",
      "bottom": "data1",
      "top": "sparse_embedding1",
      "sparse_embedding_hparam": {
        "embedding_vec_size": 128,
        "combiner": "sum",
        "workspace_size_per_gpu_in_mb": 798
      }
    },
    {
      "bottom": "dense",
      "mlp_param": {
        "activation": "Relu",
        "num_output": 1,
        "num_outputs": [
          512,
          256,
          128
        ],
        "use_bias": true
      },
      "top": "mlp1",
      "type": "MLP"
    },
    {
      "bottom": [
        "mlp1",
        "sparse_embedding1"
      ],
      "top": [
        "interaction1",
        "interaction_grad"
      ],
      "type": "Interaction"
    },
    {
      "bottom": [
        "interaction1",
        "interaction_grad"
      ],
      "mlp_param": {
        "activations": [
          "Relu",
          "Relu",
          "Relu",
          "Relu",
          "None"
        ],
        "num_output": 1,
        "num_outputs": [
          1024,
          1024,
          512,
          256,
          1
        ],
        "use_bias": true
      },
      "top": "mlp2",
      "type": "MLP"
    },
    {
      "bottom": [
        "mlp2",
        "label"
      ],
      "top": "loss",
      "type": "BinaryCrossEntropyLoss"
    }
  ]
}