{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "363b124c",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-hps-tensorflow-triton-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HPS TensorRT Plugin Benchmark for TensorFlow Large Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6370c1d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to benchmark the HPS-integrated TensorRT engine for the TensorFlow large model.\n",
    "\n",
    "For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/index.html).\n",
    "\n",
    "1. [Create TF Create the TF model](#section-1).\n",
    "2. [Build the HPS-integrated TensorRT engine](#section-2)\n",
    "3. [Benchmark HPS-integrated TensorRT engine on Triton](#section-3)\n",
    "4. [Benchmark HPS-integrated TensorRT engine on **Grace and Hooper**](#section-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "895a6471",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Use NGC\n",
    "\n",
    "The HPS TensorRT plugin is preinstalled in the 23.05 and later [Merlin TensorFlow Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow): `nvcr.io/nvidia/merlin/merlin-tensorflow:23.05`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "plugin_lib_name = \"/usr/local/hps_trt/lib/libhps_plugin.so\"\n",
    "plugin_handle = ctypes.CDLL(plugin_lib_name, mode=ctypes.RTLD_GLOBAL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bb1e540",
   "metadata": {},
   "source": [
    "<a id=\"section-1\"></a>\n",
    "## 1. Create the TF model\n",
    "\n",
    "We define the model graph with native TF layers, i.e., `tf.nn.embedding_lookup`, `tf.keras.layers.Dense` and so on. The embedding lookup layer is a placeholder here, which will be replaced by HPS plugin later to support looking up 147GB embedding table efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144ae7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 02:22:18.552734: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8859b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteModel(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 init_tensors,\n",
    "                 embed_vec_size,\n",
    "                 slot_num,\n",
    "                 dense_dim,\n",
    "                 **kwargs):\n",
    "        super(LiteModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.init_tensors = init_tensors\n",
    "        self.params = tf.Variable(initial_value=tf.concat(self.init_tensors, axis=0))\n",
    "        \n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.slot_num = slot_num\n",
    "        self.dense_dim = dense_dim\n",
    "\n",
    "        self.concat1 = tf.keras.layers.Concatenate(axis=1, name = \"concat1\")\n",
    "        self.fc1 = tf.keras.layers.Dense(1024, activation=None, name=\"fc1\")\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation=None, name=\"fc2\")\n",
    "        self.fc3 = tf.keras.layers.Dense(1, activation=None, name=\"fc3\")            \n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        categorical_features = inputs[\"categorical_features\"]\n",
    "        numerical_features = inputs[\"numerical_features\"]\n",
    "        \n",
    "        embedding_vector = tf.nn.embedding_lookup(params=self.params, ids=categorical_features)\n",
    "        reduced_embedding = tf.math.reduce_mean(embedding_vector, axis=1, keepdims=False)\n",
    "        concat_features = self.concat1([reduced_embedding, numerical_features])\n",
    "        \n",
    "        logit = self.fc3(self.fc2(self.fc1(concat_features)))\n",
    "        return logit\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = {\"categorical_features\": tf.keras.Input(shape=(self.slot_num, ), dtype=tf.int32, name=\"categorical_features\"), \n",
    "                  \"numerical_features\": tf.keras.Input(shape=(self.dense_dim, ), dtype=tf.float32, name=\"numrical_features\")}\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69760091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.compat.v1.nn.embedding_lookup_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(10000, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " categorical_features (InputLay  [(None, 26)]        0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.nn.embedding_look  (None, 26, 128)     0           ['categorical_features[0][0]']   \n",
      " up_1 (TFOpLambda)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 128)         0           ['tf.compat.v1.nn.embedding_looku\n",
      " bda)                                                            p_1[0][0]']                      \n",
      "                                                                                                  \n",
      " numrical_features (InputLayer)  [(None, 13)]        0           []                               \n",
      "                                                                                                  \n",
      " concat1 (Concatenate)          (None, 141)          0           ['tf.math.reduce_mean_1[0][0]',  \n",
      "                                                                  'numrical_features[0][0]']      \n",
      "                                                                                                  \n",
      " fc1 (Dense)                    (None, 1024)         145408      ['concat1[0][0]']                \n",
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 256)          262400      ['fc1[0][0]']                    \n",
      "                                                                                                  \n",
      " fc3 (Dense)                    (None, 1)            257         ['fc2[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408,065\n",
      "Trainable params: 408,065\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: 3fc_light.savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "# This is the placeholder embedding table. The real embedding table is of 147GB\n",
    "init_tensors = np.ones(shape=[10000, 128], dtype=np.float32)\n",
    "model = LiteModel(init_tensors, 128, 26, 13, name = \"dlrm\")\n",
    "model.summary()\n",
    "categorical_features = np.random.randint(0,100, (4096,26))\n",
    "numerical_features = np.random.random((4096, 13))\n",
    "inputs = {\"categorical_features\": categorical_features, \"numerical_features\": numerical_features}\n",
    "model(inputs)\n",
    "model.save(\"3fc_light.savedmodel\")\n",
    "\n",
    "# Release the occupied GPU memory by TensorFlow and Keras\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cadc181",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "## 2. Build the HPS-integrated TensorRT engine\n",
    "In order to use HPS in the inference stage, we create JSON configuration file and leverage the 147GB embedding table files in the HPS format.\n",
    "\n",
    "Then we convert the TF saved model to ONNX, and employ the ONNX GraphSurgoen tool to replace the native TF embedding lookup layer with the placeholder of HPS TensorRT plugin layer.\n",
    "\n",
    "After that, we can build the TensorRT engine, which is comprised of the HPS TensorRT plugin layer and the dense network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99abfe79",
   "metadata": {},
   "source": [
    "<a id=\"section-2_1\"></a>\n",
    "### Step1: Prepare the 147GB embedding table\n",
    "\n",
    "The current instructions below assume  you need to train a 147GB DLRM model from scratch based [DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/Recommendation/DLRM_and_DCNv2) using the 1TB criteo dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0525cd2",
   "metadata": {},
   "source": [
    "#### 1.1 Train a 147GB model from scratch\n",
    "Please refer to the [Quick Start Guide for DLRM](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow2/Recommendation/DLRM_and_DCNv2/doc/DLRM.md#quick-start-guide), which contains the following important steps:\n",
    "1. Built the training docker container. \n",
    "2. Preprocessing the training datasets.   \n",
    "Here you have two options, you can use the 1TB Criteo dataset to train a 147GB model (need to use Spark for data preprocessing), or you can use a synthetic dataset (avoid the preprocessing process) for fast verification.   \n",
    "    2.1. [Preprocess the Criteo 1TB dataset](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow2/Recommendation/DLRM_and_DCNv2/doc/criteo_dataset.md#quick-start-guide)   \n",
    "            *A*. `Please change the frequency limit to` *`2`* in the step `6 .Preprocess the data` to generate a training dataset for the 147GB embedding model file.  \n",
    "            *B*. Please ensure that the *`final_output_dir`* path is consistent with the input data path in the Part 3 for `step 3 Prepare the benchmark input data`.  \n",
    "        Note: The frequency limit is used to filter out the categorical values which appear less than `n` times in the whole dataset, and make them be 0. Change this variable to 1 to enable it. The default frequency limit is 15 in the script. You also can change the number as you want by changing the line of OPTS=\"--frequency_limit 8\".  \n",
    "\n",
    "    2.2  [Generate synthetic data in the same format as Criteo](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow2/Recommendation/DLRM_and_DCNv2/doc/DLRM.md#quick-start-guide)  \n",
    "    Downloading and preprocessing the Criteo 1TB dataset requires a lot of time and disk space. Because of this we provide a synthetic dataset generator that roughly matches Criteo 1TB characteristics. This will enable you to benchmark quickly.\n",
    "\n",
    "\n",
    "3. Run the training and saved a model checkpoint. If you haven't completed those steps.  \n",
    "Note:If the model is successfully saved, you will find that each category feature of the Criteo data will export a `feauture_*.npy` file, which is the embedding table for each feature, and you will merge these npy files into a complete binary embedding table for HPS in the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eee9b2d1",
   "metadata": {},
   "source": [
    "#### 1.2 Get the embedding model file in hps format\n",
    "After completing the model training and getting the 147GB embedding model file, you need to convert the embedding table  file to the HPS-format embedding file.\n",
    "In addition: you only need to complete the first three steps in the [Quick Start Guide](https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow2/Recommendation/DLRM_and_DCNv2/doc/merlin_hps_inference.md#quick-start-guide) to obtain the HPS-format embedding file.\n",
    "1.  Build the Merlin HPS docker container.\n",
    "2.  Run the training docker container built during the training stage.\n",
    "3.  Convert the model checkpoint into a Triton model repository.\n",
    "\n",
    "Then you will find a folder named `sparse` under your deploy_path(The paths provided in steps 2 and 3) for format conversion, from which you can find the two embedding table files(`emb_vector` and `key`) in HPS format. \n",
    "\n",
    "4. Copy the above two files(`emb_vector` and `key`) to the deployment path (model_repo/hps_model/1/dlrm_sparse.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e7eb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_repo/hps_model/1/dlrm_sparse.model\r\n",
      "├── memb_vector\r\n",
      "└── mkey\r\n",
      "\r\n",
      "0 directories, 2 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree -n model_repo/hps_model/1/dlrm_sparse.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4da8de38",
   "metadata": {},
   "source": [
    "<a id=\"section-2.2\"></a>\n",
    "### Step2: Prepare JSON configuration file for HPS\n",
    "\n",
    "Please note that the storage format in the `dlrm_sparse.model/key` file is int64, while the HPS TensorRT plugin currently only support int32 when loading the keys into memory.  \n",
    "Note:In order to facilitate the benchmark test from the minimum batch to the maximum batch, we set the test range to a maximum of 65536. If you want to get better performance, please set each batch independently. For instance, if you set batch=32, it is only used to the case with 32 samples for one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6288d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing light.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile light.json\n",
    "{\n",
    "    \"supportlonglong\": false,\n",
    "    \"models\": [{\n",
    "        \"model\": \"light\",\n",
    "        \"sparse_files\": [\"/hugectr/hps_trt/notebooks/model_repo/hps_model/1/dlrm_sparse.model\"],\n",
    "        \"num_of_worker_buffer_in_pool\": 1,\n",
    "        \"num_of_refresher_buffer_in_pool\": 0,\n",
    "        \"embedding_table_names\":[\"sparse_embedding1\"],\n",
    "        \"embedding_vecsize_per_table\": [128],\n",
    "        \"maxnum_catfeature_query_per_table_per_sample\": [26],\n",
    "        \"default_value_for_each_table\": [1.0],\n",
    "        \"deployed_device_list\": [0],\n",
    "        \"max_batch_size\": 65536,\n",
    "        \"cache_refresh_percentage_per_iteration\": 0.0,\n",
    "        \"hit_rate_threshold\": 1.0,\n",
    "        \"gpucacheper\": 0.00,\n",
    "        \"gpucache\": true,\n",
    "        \"init_ec\": false,\n",
    "        \"embedding_cache_type\": \"static\",\n",
    "        \"enable_pagelock\" = true\n",
    "        \"use_context_stream\": true\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5306455e",
   "metadata": {},
   "source": [
    "### Step3: Convert to ONNX and do ONNX graph surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415d4a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-08 02:28:29.577492: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2023-06-08 02:28:32,462 - WARNING - ***IMPORTANT*** Installed protobuf is not cpp accelerated. Conversion will be extremely slow. See https://github.com/onnx/tensorflow-onnx/issues/1557\n",
      "2023-06-08 02:28:36,132 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2023-06-08 02:28:36,928 - INFO - Signatures found in model: [serving_default].\n",
      "2023-06-08 02:28:36,928 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2023-06-08 02:28:36,928 - INFO - Output names: ['output_1']\n",
      "2023-06-08 02:28:37,440 - INFO - Using tensorflow=2.11.0, onnx=1.14.0, tf2onnx=1.14.0/8f8d49\n",
      "2023-06-08 02:28:37,440 - INFO - Using opset <onnx, 15>\n",
      "2023-06-08 02:28:37,459 - INFO - Computed 0 values for constant folding\n",
      "2023-06-08 02:28:37,482 - INFO - Optimizing ONNX model\n",
      "2023-06-08 02:28:37,541 - INFO - After optimization: Const -3 (7->4), Identity -2 (2->0)\n",
      "2023-06-08 02:28:37,781 - INFO - \n",
      "2023-06-08 02:28:37,781 - INFO - Successfully converted TensorFlow model 3fc_light.savedmodel to ONNX\n",
      "2023-06-08 02:28:37,781 - INFO - Model inputs: ['categorical_features', 'numerical_features']\n",
      "2023-06-08 02:28:37,781 - INFO - Model outputs: ['output_1']\n",
      "2023-06-08 02:28:37,781 - INFO - ONNX model is saved at 3fc_light.onnx\n"
     ]
    }
   ],
   "source": [
    "# convert TF SavedModel to ONNX\n",
    "!python -m tf2onnx.convert --saved-model 3fc_light.savedmodel --output 3fc_light.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2264c2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] colored module is not installed, will not use colors when logging. To enable colors, please install the colored module: python3 -m pip install colored\n",
      "[W] Found distinct tensors that share the same name:\n",
      "[id: 139822124016208] Variable (categorical_features): (shape=('unknown', 26), dtype=<class 'numpy.int32'>)\n",
      "[id: 139821990953120] Variable (categorical_features): (shape=['unk__6', 26], dtype=int64)\n",
      "Note: Producer node(s) of first tensor:\n",
      "[]\n",
      "Producer node(s) of second tensor:\n",
      "[]\n",
      "[W] colored module is not installed, will not use colors when logging. To enable colors, please install the colored module: python3 -m pip install colored\n",
      "[W] Found distinct tensors that share the same name:\n",
      "[id: 139821990953120] Variable (categorical_features): (shape=['unk__6', 26], dtype=int64)\n",
      "[id: 139822124016208] Variable (categorical_features): (shape=('unknown', 26), dtype=<class 'numpy.int32'>)\n",
      "Note: Producer node(s) of first tensor:\n",
      "[]\n",
      "Producer node(s) of second tensor:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    " # ONNX graph surgery to insert HPS the TensorRT plugin placeholder\n",
    "import onnx_graphsurgeon as gs\n",
    "from onnx import  shape_inference\n",
    "import numpy as np\n",
    "import onnx\n",
    "\n",
    "graph = gs.import_onnx(onnx.load(\"3fc_light.onnx\"))\n",
    "saved = []\n",
    "\n",
    "for node in graph.nodes:\n",
    "    if node.name == \"StatefulPartitionedCall/dlrm/embedding_lookup\":\n",
    "        categorical_features = gs.Variable(name=\"categorical_features\", dtype=np.int32, shape=(\"unknown\", 26))\n",
    "        hps_node = gs.Node(op=\"HPS_TRT\", attrs={\"ps_config_file\": \"light.json\\0\", \"model_name\": \"light\\0\", \"table_id\": 0, \"emb_vec_size\": 128}, \n",
    "                           inputs=[categorical_features], outputs=[node.outputs[0]])\n",
    "        graph.nodes.append(hps_node)\n",
    "        saved.append(categorical_features)\n",
    "        node.outputs.clear()\n",
    "for i in graph.inputs:\n",
    "    if i.name == \"numerical_features\":\n",
    "        saved.append(i)\n",
    "graph.inputs = saved\n",
    "\n",
    "graph.cleanup().toposort()\n",
    "onnx.save(gs.export_onnx(graph), \"3fc_light_with_hps.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "996a8e18",
   "metadata": {},
   "source": [
    "### Step4: Build the TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9896384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPS_TRT\n",
      "[06/08/2023-02:37:03] [TRT] [I] [MemUsageChange] Init CUDA: CPU +974, GPU +0, now: CPU 2531, GPU 661 (MiB)\n",
      "[06/08/2023-02:37:09] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +336, GPU +74, now: CPU 2943, GPU 735 (MiB)\n",
      "[06/08/2023-02:37:09] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[06/08/2023-02:37:09] [TRT] [I] No importer registered for op: HPS_TRT. Attempting to import as plugin.\n",
      "[06/08/2023-02:37:09] [TRT] [I] Searching for plugin: HPS_TRT, plugin_version: 1, plugin_namespace: \n",
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][02:37:09.116][INFO][RK0][main]: fuse_embedding_table is not specified using default: 0\n",
      "[HCTR][02:37:09.116][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: use_static_table is not specified using default: 0\n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: use_hctr_cache_implementation is not specified using default: 1\n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: HPS plugin uses context stream for model light: True\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][02:37:09.117][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][02:37:09.117][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][02:37:09.117][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][02:37:09.177][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: Model name: light\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: Max batch size: 65536\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: Fuse embedding tables: False\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: Use static table: False\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: Use I64 input key: False\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: The size of worker memory pool: 1\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: The size of refresh memory pool: 0\n",
      "[HCTR][02:37:09.177][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "[HCTR][02:38:09.140][INFO][RK0][main]: Initialize the embedding cache by by inserting the same size model file with embedding cache from beginning\n",
      "[HCTR][02:38:09.140][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][02:38:09.141][INFO][RK0][main]: EC initialization on device 0 for hps_et.light.sparse_embedding1\n",
      "[HCTR][03:41:57.227][INFO][RK0][main]: LookupSession i64_input_key: False\n",
      "[HCTR][03:41:57.227][INFO][RK0][main]: Creating lookup session for light on device: 0\n",
      "[06/08/2023-03:41:57] [TRT] [I] Successfully created plugin: HPS_TRT\n",
      "9\n",
      "[06/08/2023-03:41:57] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[06/08/2023-03:41:57] [TRT] [I] Graph optimization time: 0.0088975 seconds.\n",
      "[06/08/2023-03:41:57] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 2952, GPU 12129 (MiB)\n",
      "[06/08/2023-03:41:57] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +349, GPU +190, now: CPU 3301, GPU 12319 (MiB)\n",
      "[06/08/2023-03:41:57] [TRT] [W] TensorRT was linked against cuDNN 8.9.0 but loaded cuDNN 8.7.0\n",
      "[06/08/2023-03:41:57] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[06/08/2023-03:41:57] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/08/2023-03:42:03] [TRT] [I] Detected 2 inputs and 1 output network tensors.\n",
      "[06/08/2023-03:42:03] [TRT] [I] Total Host Persistent Memory: 16672\n",
      "[06/08/2023-03:42:03] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[06/08/2023-03:42:03] [TRT] [I] Total Scratch Memory: 0\n",
      "[06/08/2023-03:42:03] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 3 MiB, GPU 1248 MiB\n",
      "[06/08/2023-03:42:03] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 10 steps to complete.\n",
      "[06/08/2023-03:42:03] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.040758ms to assign 3 blocks to 10 nodes requiring 905970176 bytes.\n",
      "[06/08/2023-03:42:03] [TRT] [I] Total Activation Memory: 905969664\n",
      "[06/08/2023-03:42:03] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3302, GPU 12397 (MiB)\n",
      "[06/08/2023-03:42:03] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 3302, GPU 12407 (MiB)\n",
      "[06/08/2023-03:42:03] [TRT] [W] TensorRT was linked against cuDNN 8.9.0 but loaded cuDNN 8.7.0\n",
      "[06/08/2023-03:42:03] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[06/08/2023-03:42:03] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[06/08/2023-03:42:03] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[06/08/2023-03:42:03] [TRT] [W] - 2 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[06/08/2023-03:42:03] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import ctypes\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "def create_hps_plugin_creator():\n",
    "    trt_version = [int(n) for n in trt.__version__.split('.')]\n",
    "\n",
    "    plugin_lib_name = \"/usr/local/hps_trt/lib/libhps_plugin.so\"\n",
    "    handle = ctypes.CDLL(plugin_lib_name, mode=ctypes.RTLD_GLOBAL)\n",
    "\n",
    "    trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "    plg_registry = trt.get_plugin_registry()\n",
    "\n",
    "    for plugin_creator in plg_registry.plugin_creator_list:\n",
    "        if plugin_creator.name[0] == \"H\":\n",
    "            print(plugin_creator.name)\n",
    "\n",
    "    hps_plugin_creator = plg_registry.get_plugin_creator(\"HPS_TRT\", \"1\", \"\")\n",
    "    return hps_plugin_creator\n",
    "\n",
    "def build_engine_from_onnx(onnx_model_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(EXPLICIT_BATCH) as network, trt.OnnxParser(network, TRT_LOGGER) as parser, builder.create_builder_config() as builder_config:        \n",
    "        model = open(onnx_model_path, 'rb')\n",
    "        parser.parse(model.read())\n",
    "        print(network.num_layers)\n",
    "        \n",
    "        builder_config.set_flag(trt.BuilderFlag.FP16)\n",
    "        profile = builder.create_optimization_profile()        \n",
    "        profile.set_shape(\"categorical_features\", (1, 26), (1024, 26), (65536, 26))    \n",
    "        profile.set_shape(\"numerical_features\", (1, 13), (1024, 13), (65536, 13))\n",
    "        builder_config.add_optimization_profile(profile)\n",
    "\n",
    "        engine = builder.build_serialized_network(network, builder_config)\n",
    " \n",
    "        return engine\n",
    "\n",
    "create_hps_plugin_creator()\n",
    "serialized_engine = build_engine_from_onnx(\"3fc_light_with_hps.onnx\")\n",
    "with open(\"dynamic_3fc_light.trt\", \"wb\") as fout:\n",
    "    fout.write(serialized_engine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03b3aa9e",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "## 3. Benchmark HPS-integrated TensorRT engine on Triton"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8673bbc5",
   "metadata": {},
   "source": [
    "### Step1: Create the model repository\n",
    "\n",
    "In order to benchmark the TensorRT engine with the Triton TensorRT backend, we need to create the model repository and define the `config.pbtxt` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b038b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repo/dynamic_3fc_lite_hps_trt/1\n",
    "!mv dynamic_3fc_light.trt model_repo/dynamic_3fc_lite_hps_trt/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af4223a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_repo/dynamic_3fc_lite_hps_trt/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_repo/dynamic_3fc_lite_hps_trt/config.pbtxt\n",
    "\n",
    "platform: \"tensorrt_plan\"\n",
    "default_model_filename: \"dynamic_3fc_light.trt\"\n",
    "backend: \"tensorrt\"\n",
    "max_batch_size: 0\n",
    "input [\n",
    "  {\n",
    "    name: \"categorical_features\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [-1,26]\n",
    "  },\n",
    "  {\n",
    "    name: \"numerical_features\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1,13]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "      name: \"output_1\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1,1]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus:[0]\n",
    "\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6ebac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmodel_repo/dynamic_3fc_lite_hps_trt\r\n",
      "├── 1\r\n",
      "│   └── dynamic_3fc_light.trt\r\n",
      "└── config.pbtxt\r\n",
      "\r\n",
      "1 directory, 2 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree -n model_repo/dynamic_3fc_lite_hps_trt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07a3cddc",
   "metadata": {},
   "source": [
    "### Step2: Prepare the benchmark input data\n",
    "\n",
    "To benchmark with [Triton Performance Analyzer](https://github.com/triton-inference-server/client/tree/main/src/c%2B%2B/perf_analyzer), we need to prepare the input data of the required format based on Criteo dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1dbe436",
   "metadata": {},
   "source": [
    "In [part 2 section 1.1](#section-2_1), we have created the binary dataset in `final_output_dir`. We provide you with a [script](./spark2json.py) to convert this binary data into JSON format that can be fed into the Triton Performance Analyzer. To use the script, you will need the [DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/Recommendation/DLRM_and_DCNv2) again. Make sure you have add it to your $PYTHONPATH.\n",
    "\n",
    "If not, please run `export PYTHONPATH=/DeepLearningExamples/TensorFlow2/Recommendation/DLRM_and_DCNv2`. **Remember to replace the path with the correct path in your workspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dir to store the JSON format data\n",
    "!mkdir -p ./perf_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b966cf64",
   "metadata": {},
   "source": [
    "**Please note that the following script will takes several minutes to finish.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5bd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the Python script to convert binary data to JSON format\n",
    "!python spark2json.py --result-path ./perf_data --dataset_path /path/to/your/binary_split_converted_data --num-benchmark-samples 2000000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bdebae2",
   "metadata": {},
   "source": [
    "**Remember to replace the `--dataset_path` with the correct path in your workspace, and specify the `--num-benchmark-samples` you want to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab87b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perf_data\r\n",
      "├── 1024.json\r\n",
      "├── 16384.json\r\n",
      "├── 2048.json\r\n",
      "├── 256.json\r\n",
      "├── 32768.json\r\n",
      "├── 4096.json\r\n",
      "├── 512.json\r\n",
      "├── 65536.json\r\n",
      "└── 8192.json\r\n",
      "\r\n",
      "0 directories, 9 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree -n perf_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6869f60",
   "metadata": {},
   "source": [
    "### Step3: Launch the Triton inference server\n",
    "\n",
    "We can then launch the Triton inference server using the TensorRT backend. Please note that `LD_PRELOAD` is utilized to load the custom TensorRT plugin (i.e., HPS TensorRT plugin) into Triton.\n",
    "\n",
    "Note: `Since Background processes not supported by Jupyter, please launch the Triton Server according to the following command independently in the background`.\n",
    "\n",
    "> **LD_PRELOAD=/usr/local/hps_trt/lib/libhps_plugin.so tritonserver --model-repository=/hugectr/hps_trt/notebooks/model_repo/ --load-model=dynamic_3fc_lite_hps_trt --model-control-mode=explicit**\n",
    "\n",
    "If you successfully started tritonserver, you should see a log similar to following:\n",
    "\n",
    "\n",
    "```bash\n",
    "+--------------------------+---------+--------+\n",
    "| Model                    | Version | Status |\n",
    "+--------------------------+---------+--------+\n",
    "| dynamic_3fc_lite_hps_trt | 1       | READY  |\n",
    "+--------------------------+---------+--------+\n",
    "\n",
    "Started GRPCInferenceService at 0.0.0.0:8001\n",
    "Started HTTPService at 0.0.0.0:8000\n",
    "Started Metrics Service at 0.0.0.0:8002\n",
    "```\n",
    "\n",
    "We can then benchmark the online inference performance of this HPS-integrated engine with 147GB embedding table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4832af66",
   "metadata": {},
   "source": [
    "### Step4: Run the benchmark\n",
    "\n",
    "The reported `compute infer` number at the server side is the end-to-end inference latency of the engine, which covers HPS embedding lookup and TensorRT forward of dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b8ae835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting benchmark.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmark.sh\n",
    "\n",
    "batch_size=(256 512 1024 2048 4096 8192 16384 32768 65536)\n",
    "\n",
    "model_name=(\"dynamic_3fc_lite_hps_trt\")\n",
    "\n",
    "for b in ${batch_size[*]};\n",
    "do\n",
    "  for m in ${model_name[*]};\n",
    "  do\n",
    "    echo $b $m\n",
    "    perf_analyzer -m ${m} -u localhost:8000 --input-data perf_data/${b}.json --shape categorical_features:${b},26 --shape numerical_features:${b},13\n",
    "  done\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1dc63c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 25600 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 20941\n",
      "    Throughput: 1163.14 infer/sec\n",
      "    Avg latency: 851 usec (standard deviation 1184 usec)\n",
      "    p50 latency: 799 usec\n",
      "    p90 latency: 922 usec\n",
      "    p95 latency: 977 usec\n",
      "    p99 latency: 1190 usec\n",
      "    Avg HTTP time: 846 usec (send/recv 85 usec + response wait 761 usec)\n",
      "  Server: \n",
      "    Inference count: 20941\n",
      "    Execution count: 20941\n",
      "    Successful request count: 20941\n",
      "    Avg request latency: 551 usec (overhead 21 usec + queue 40 usec + compute input 38 usec + compute infer 343 usec + compute output 108 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 1163.14 infer/sec, latency 851 usec\n",
      "512 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 12800 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 14135\n",
      "    Throughput: 785.143 infer/sec\n",
      "    Avg latency: 1264 usec (standard deviation 286 usec)\n",
      "    p50 latency: 1236 usec\n",
      "    p90 latency: 1340 usec\n",
      "    p95 latency: 1374 usec\n",
      "    p99 latency: 1476 usec\n",
      "    Avg HTTP time: 1258 usec (send/recv 92 usec + response wait 1166 usec)\n",
      "  Server: \n",
      "    Inference count: 14135\n",
      "    Execution count: 14135\n",
      "    Successful request count: 14135\n",
      "    Avg request latency: 889 usec (overhead 27 usec + queue 44 usec + compute input 42 usec + compute infer 619 usec + compute output 156 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 785.143 infer/sec, latency 1264 usec\n",
      "1024 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 6400 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 8116\n",
      "    Throughput: 450.826 infer/sec\n",
      "    Avg latency: 2206 usec (standard deviation 391 usec)\n",
      "    p50 latency: 2183 usec\n",
      "    p90 latency: 2321 usec\n",
      "    p95 latency: 2368 usec\n",
      "    p99 latency: 2486 usec\n",
      "    Avg HTTP time: 2199 usec (send/recv 118 usec + response wait 2081 usec)\n",
      "  Server: \n",
      "    Inference count: 8116\n",
      "    Execution count: 8116\n",
      "    Successful request count: 8116\n",
      "    Avg request latency: 1632 usec (overhead 45 usec + queue 73 usec + compute input 106 usec + compute infer 1173 usec + compute output 234 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 450.826 infer/sec, latency 2206 usec\n",
      "2048 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 3200 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 5311\n",
      "    Throughput: 295.01 infer/sec\n",
      "    Avg latency: 3377 usec (standard deviation 459 usec)\n",
      "    p50 latency: 3349 usec\n",
      "    p90 latency: 3486 usec\n",
      "    p95 latency: 3530 usec\n",
      "    p99 latency: 3820 usec\n",
      "    Avg HTTP time: 3370 usec (send/recv 155 usec + response wait 3215 usec)\n",
      "  Server: \n",
      "    Inference count: 5311\n",
      "    Execution count: 5311\n",
      "    Successful request count: 5311\n",
      "    Avg request latency: 2591 usec (overhead 50 usec + queue 76 usec + compute input 162 usec + compute infer 2068 usec + compute output 234 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 295.01 infer/sec, latency 3377 usec\n",
      "4096 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 1600 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 3518\n",
      "    Throughput: 195.42 infer/sec\n",
      "    Avg latency: 5109 usec (standard deviation 380 usec)\n",
      "    p50 latency: 5068 usec\n",
      "    p90 latency: 5242 usec\n",
      "    p95 latency: 5316 usec\n",
      "    p99 latency: 5741 usec\n",
      "    Avg HTTP time: 5104 usec (send/recv 171 usec + response wait 4933 usec)\n",
      "  Server: \n",
      "    Inference count: 3518\n",
      "    Execution count: 3518\n",
      "    Successful request count: 3518\n",
      "    Avg request latency: 4134 usec (overhead 38 usec + queue 48 usec + compute input 138 usec + compute infer 3742 usec + compute output 167 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 195.42 infer/sec, latency 5109 usec\n",
      "8192 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 800 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 1910\n",
      "    Throughput: 106.097 infer/sec\n",
      "    Avg latency: 9412 usec (standard deviation 553 usec)\n",
      "    p50 latency: 9384 usec\n",
      "    p90 latency: 9529 usec\n",
      "    p95 latency: 9581 usec\n",
      "    p99 latency: 10106 usec\n",
      "    Avg HTTP time: 9406 usec (send/recv 294 usec + response wait 9112 usec)\n",
      "  Server: \n",
      "    Inference count: 1910\n",
      "    Execution count: 1910\n",
      "    Successful request count: 1910\n",
      "    Avg request latency: 7674 usec (overhead 42 usec + queue 54 usec + compute input 267 usec + compute infer 7179 usec + compute output 130 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 106.097 infer/sec, latency 9412 usec\n",
      "16384 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 400 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 992\n",
      "    Throughput: 55.1033 infer/sec\n",
      "    Avg latency: 18132 usec (standard deviation 726 usec)\n",
      "    p50 latency: 18051 usec\n",
      "    p90 latency: 18257 usec\n",
      "    p95 latency: 18330 usec\n",
      "    p99 latency: 23069 usec\n",
      "    Avg HTTP time: 18125 usec (send/recv 1278 usec + response wait 16847 usec)\n",
      "  Server: \n",
      "    Inference count: 992\n",
      "    Execution count: 992\n",
      "    Successful request count: 992\n",
      "    Avg request latency: 14999 usec (overhead 29 usec + queue 56 usec + compute input 476 usec + compute infer 14234 usec + compute output 203 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 55.1033 infer/sec, latency 18132 usec\n",
      "32768 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 200 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 515\n",
      "    Throughput: 28.6081 infer/sec\n",
      "    Avg latency: 34878 usec (standard deviation 927 usec)\n",
      "    p50 latency: 34734 usec\n",
      "    p90 latency: 35143 usec\n",
      "    p95 latency: 35288 usec\n",
      "    p99 latency: 40804 usec\n",
      "    Avg HTTP time: 34872 usec (send/recv 2584 usec + response wait 32288 usec)\n",
      "  Server: \n",
      "    Inference count: 516\n",
      "    Execution count: 516\n",
      "    Successful request count: 516\n",
      "    Avg request latency: 29340 usec (overhead 33 usec + queue 55 usec + compute input 870 usec + compute infer 28111 usec + compute output 270 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 28.6081 infer/sec, latency 34878 usec\n",
      "65536 dynamic_3fc_lite_hps_trt\n",
      " Successfully read data for 1 stream/streams with 100 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Client: \r\n",
      "    Request count: 253\r\n",
      "    Throughput: 14.053 infer/sec\r\n",
      "    Avg latency: 71063 usec (standard deviation 1570 usec)\r\n",
      "    p50 latency: 70749 usec\r\n",
      "    p90 latency: 71666 usec\r\n",
      "    p95 latency: 73226 usec\r\n",
      "    p99 latency: 77979 usec\r\n",
      "    Avg HTTP time: 71058 usec (send/recv 5092 usec + response wait 65966 usec)\r\n",
      "  Server: \r\n",
      "    Inference count: 253\r\n",
      "    Execution count: 253\r\n",
      "    Successful request count: 253\r\n",
      "    Avg request latency: 60716 usec (overhead 38 usec + queue 58 usec + compute input 1804 usec + compute infer 58482 usec + compute output 333 usec)\r\n",
      "\r\n",
      "Inferences/Second vs. Client Average Batch Latency\r\n",
      "Concurrency: 1, throughput: 14.053 infer/sec, latency 71063 usec\r\n"
     ]
    }
   ],
   "source": [
    "!bash benchmark.sh > result.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38788ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./summary.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./summary.py\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "log_pattern = {\n",
    "    \"inference_benchmark\": {\n",
    "        \"cmd_log\": r\"compute infer\",\n",
    "        \"result_log\": r\"compute infer (\\d+\\.?\\d*) usec\",\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def extract_result_from_log(log_path):\n",
    "    job_log_pattern = log_pattern[\"inference_benchmark\"]\n",
    "    results = []\n",
    "    with open(log_path, \"r\", errors=\"ignore\") as f:\n",
    "        lines = \"\".join(f.readlines())\n",
    "        job_logs = lines.split(\"+ \")\n",
    "        for each_job_log in job_logs:\n",
    "            if re.search(job_log_pattern[\"cmd_log\"], each_job_log):\n",
    "                for line in each_job_log.split(\"\\n\"):\n",
    "                    match = re.search(job_log_pattern[\"result_log\"], line)\n",
    "                    if match is None:\n",
    "                        continue\n",
    "                    result = float(match.group(1))\n",
    "                    results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--log_path\", required=True)\n",
    "    args = parser.parse_args()\n",
    "    batch_sizes = [\"256\", \"512\", \"1024\", \"2048\", \"4096\", \"8192\", \"16384\", \"32768\", \"65536\"]\n",
    "\n",
    "\n",
    "    perf_result = extract_result_from_log(args.log_path)\n",
    "    idx = 0\n",
    "    batch_sizes = [\"256\", \"512\", \"1024\", \"2048\", \"4096\", \"8192\", \"16384\", \"32768\", \"65536\"]\n",
    "    print(\"Inference Latency (usec)\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print(\"batch_size\\tresult \\t\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    for i in range(len(perf_result)):\n",
    "        print(\"{}\\t\\t{}\\t\\t\".format(\n",
    "                        batch_sizes[i],\n",
    "                        perf_result[i]\n",
    "                    )\n",
    "                )\n",
    "        print(\n",
    "                \"-----------------------------------------------------------------------------------------\"\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bc9e67d",
   "metadata": {},
   "source": [
    "*Summarize the result*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8dd0839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency (usec)\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "batch_size\tresult \t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "256\t\t343.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "512\t\t619.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "1024\t\t1173.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "2048\t\t2068.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "4096\t\t3742.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "8192\t\t7179.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "16384\t\t14234.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "32768\t\t28111.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "65536\t\t58482.0\t\t\r\n",
      "-----------------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python ./summary.py --log_path result.log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06b8c31a",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "## 4. Benchmark for ARM64 or Grace + Hooper systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "590b4902",
   "metadata": {},
   "source": [
    "Our prebuilt Grace-optimized ARM64 images are currently undergoing testing, and are therefore not yet available via NGC. This will change soon. If you want to benchmark on ARM and in particular a system equipped with a NVIDIA Grace CPU, you can build a compatible `docker` image yourself by following these steps.\n",
    "\n",
    "In some steps we provide 2 mutually exclusive alternatives.\n",
    "- **Option A (portable ARM64 HugeCTR)**: If you follow *option A* instructions, you will build the standard version of HugeCTR for ARM64 platforms. This approach produces binaries that are more portable, but may not allow you get the most out your Grace+Hopper hardware setup.\n",
    "- **Option B (G+H optimized HugeCTR)**: In contrast, if you follow *option B* instructions, you will build and run a HugeCTR variant that maximizes DLRM throughput on Grace+Hopper systems. However, please be advised that slight alterations to the system setup are necessary to achieve this. To apply these alterations you *must* have `root` access.es configuration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6edfa48b-5ed5-40b6-a307-782ad16adcd8",
   "metadata": {},
   "source": [
    "### Step 1: Build the NVIDIA Merlin docker images\n",
    "\n",
    "Use the following instructions on your ARM system to download and build `merlin-base` and `merlin-tensorflow` docker images required for the benchmark.\n",
    "\n",
    "- **Option A (portable ARM64 HugeCTR)**:\n",
    "   ```shell\n",
    "   git clone https://github.com/NVIDIA-Merlin/Merlin.git\n",
    "   cd Merlin/docker\n",
    "   docker build -t nvcr.io/nvstaging/merlin/merlin-base:23.12 -f dockerfile.merlin .\n",
    "   docker build -t nvcr.io/nvstaging/merlin/merlin-tensorflow:23.12 -f dockerfile.tf .\n",
    "   cd ../..\n",
    "   ```\n",
    "- **Option B (G+H optimized HugeCTR)**:\n",
    "   ```shell\n",
    "   git clone https://github.com/NVIDIA-Merlin/Merlin.git\n",
    "   cd Merlin/docker\n",
    "   sed -i -e 's/\" -DENABLE_INFERENCE=ON/\" -DUSE_HUGE_PAGES=ON -DENABLE_INFERENCE=ON/g' dockerfile.merlin\n",
    "   docker build -t nvcr.io/nvstaging/merlin/merlin-base:23.12 -f dockerfile.merlin .\n",
    "   docker build -t nvcr.io/nvstaging/merlin/merlin-tensorflow:23.12 -f dockerfile.tf .\n",
    "   cd ../..\n",
    "   ````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e13fd853-7c20-411b-a29c-59fa0e203ea9",
   "metadata": {},
   "source": [
    "### Step 2: Prepare host system for running the docker container\n",
    "\n",
    "- **Option A (portable ARM64 HugeCTR)**:\n",
    "  No action required.\n",
    "- **Option B (G+H optimized HugeCTR)**:\n",
    "   Adjust your Grace+Hopper system configuration to increase the number of large memory pages for the benchmark.\n",
    "   ```shell\n",
    "   sudo echo '180000' > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages\n",
    "   ```\n",
    "   This make take a while.  \n",
    "   \n",
    "   In addition, you can reuse the [light.json](#section-2.2) configuration file in [*Prepare JSON configuration file for HPS*](#section-2.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b1b77",
   "metadata": {},
   "source": [
    "### Step 3: Create the model\n",
    "\n",
    "Follow to [Create TF Create the TF model](#section-1) to create the model. There are many ways to accomplish this. We suggest simply running this Jupyter notebook using the docker image that you just created in your ARM64 / Grace+Hopper node, and forward the web-server port to the host system.\n",
    "\n",
    "Your filesystem or system environment might impose constraints. The following command just serves as an example. It assumes HugeCTR was downloaded from GitHub into the current working directory (`git clone https://github.com/NVIDIA-Merlin/HugeCTR.git`). To allow writing files, we first give root user (inside the docker image you are root) to access to the notebook folder (this folder), and then startup a suitable Jupyter server.\n",
    "```shell\n",
    "export HCTR_SRC=\"${PWD}/HugeCTR\" && chmod -R 777 \"${HCTR_SRC}/hps_trt/notebooks\" && docker run -it --rm --gpus all --network=host -v ${HCTR_SRC}:/hugectr nvcr.io/nvstaging/merlin/merlin-tensorflow:23.12 jupyter-lab --allow-root --ip 0.0.0.0 --port 8888 --no-browser --notebook-dir=/hugectr/hps_trt/notebooks\n",
    "```   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be5238dd-f39e-4430-9cf9-6ff266255470",
   "metadata": {},
   "source": [
    "### Step 4: Prepare data\n",
    "\n",
    "Next, follow the instructions in [Build the HPS-integrated TensorRT engine](#section-2) to create the dataset and the predconditions for benchmarking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "528fa761-0485-48e8-bcec-55b296f70180",
   "metadata": {},
   "source": [
    "### Step 5: Run benchmark\n",
    "\n",
    "Follow the steps outlined in [Benchmark HPS-integrated TensorRT engine on Triton](#section-3) to execute the benchmark itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbc838-2bd8-4796-8265-03a125eac71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
